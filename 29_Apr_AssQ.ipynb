{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q1. Explain the basic concept of clustering and give examples of applications where clustering is useful.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is a technique in unsupervised machine learning that involves grouping similar objects or data points together based on their inherent characteristics or patterns. The goal of clustering is to discover hidden structures within the data without any predefined labels or class information. The basic concept of clustering involves partitioning or organizing data into clusters, where objects within a cluster are more similar to each other compared to objects in other clusters.\n",
    "\n",
    "Here are a few examples of applications where clustering is useful:\n",
    "\n",
    "1. Customer Segmentation: Clustering is widely used in market segmentation to group customers based on their similarities in purchasing behavior, demographics, or preferences. This helps businesses understand different customer segments, tailor their marketing strategies, and provide personalized recommendations or offers.\n",
    "\n",
    "2. Image Segmentation: Clustering is employed in computer vision to segment images into meaningful regions or objects based on color, texture, or other visual features. It enables applications such as object recognition, image retrieval, and image-based sorting.\n",
    "\n",
    "3. Anomaly Detection: Clustering can be used for identifying anomalies or outliers in datasets. By clustering normal instances together, any data point that significantly deviates from its cluster can be considered as an anomaly. This is useful in fraud detection, network intrusion detection, and detecting manufacturing defects.\n",
    "\n",
    "4. Document Clustering: Clustering is applied to text mining and information retrieval to group similar documents based on their content. It aids in organizing large document collections, topic modeling, and document recommendation systems.\n",
    "\n",
    "5. Social Network Analysis: Clustering techniques are used to identify communities or groups within social networks. It helps in understanding social relationships, detecting influencers, and targeted advertising or content delivery.\n",
    "\n",
    "6. Biological Data Analysis: Clustering is utilized in various biological applications, such as clustering genes based on expression profiles, clustering proteins based on sequence similarity, or clustering patients based on their medical characteristics. It aids in identifying patterns, understanding diseases, and guiding treatment decisions.\n",
    "\n",
    "These are just a few examples of the diverse range of applications where clustering is useful. Clustering enables pattern discovery, data exploration, and organization, providing valuable insights and supporting decision-making in various domains."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q2. What is DBSCAN and how does it differ from other clustering algorithms such as k-means and hierarchical clustering?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together data points based on their density and proximity. Unlike k-means and hierarchical clustering, which are centroid-based or connectivity-based methods, DBSCAN takes a density-based approach to cluster formation. Here are the key characteristics of DBSCAN that differentiate it from other clustering algorithms:\n",
    "\n",
    "1. Density-Based Clustering: DBSCAN identifies clusters based on the density of data points. It defines a cluster as a region of high density surrounded by a region of low density. It is able to discover clusters of arbitrary shape and handle clusters of different sizes and densities effectively.\n",
    "\n",
    "2. No Assumption of Cluster Shape: DBSCAN does not assume any specific shape for the clusters. It can detect clusters with irregular shapes, such as elongated, convex, or overlapping clusters, as long as the density criterion is satisfied.\n",
    "\n",
    "3. Noise and Outlier Handling: DBSCAN has the ability to identify and handle noise and outliers as individual data points that do not belong to any cluster. These points are considered as noise or outliers, which is beneficial for applications where noise detection is important.\n",
    "\n",
    "4. Parameter-Free: DBSCAN does not require specifying the number of clusters in advance. It automatically determines the number of clusters based on the data and the specified parameters such as the neighborhood size and the minimum number of points required to form a dense region.\n",
    "\n",
    "5. Proximity and Density: DBSCAN uses proximity and density-based measures to determine cluster membership. It considers a point as a core point if it has a sufficient number of neighboring points within a specified distance. Points that are reachable from core points or are connected through a series of core points are assigned to the same cluster.\n",
    "\n",
    "6. Handling Uneven Density: DBSCAN can handle datasets with varying densities effectively. It can handle clusters of different sizes and densities by adapting to the local density variations in the data.\n",
    "\n",
    "In summary, DBSCAN differs from other clustering algorithms such as k-means and hierarchical clustering by taking a density-based approach, handling clusters of arbitrary shapes, being parameter-free in terms of the number of clusters, and effectively handling noise and outliers. It is particularly useful in scenarios where the data has irregular or complex cluster structures, varying densities, and where noise detection is important."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q3. How do you determine the optimal values for the epsilon and minimum points parameters in DBSCAN clustering?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the optimal values for the epsilon (ε) and minimum points parameters in DBSCAN clustering is important to ensure the desired clustering performance. Here are a few approaches that can be used to determine these parameter values:\n",
    "\n",
    "1. Domain Knowledge: Domain knowledge and understanding of the data can provide valuable insights for setting the parameters. Consider the inherent characteristics of the data, such as the expected density of the clusters, the distance between data points, and the level of noise. Knowledge about the specific problem and the data generating process can guide you in selecting appropriate parameter values.\n",
    "\n",
    "2. Visual Inspection: Visualize the data and perform exploratory data analysis to gain insights into the structure and density of the data. Plotting the data points and experimenting with different parameter values can help identify suitable values. Observe how different parameter settings affect the clustering results and ensure that they align with the expected cluster formations.\n",
    "\n",
    "3. K-Distance Plot: The k-distance plot helps in determining the optimal epsilon value. For each data point, calculate the distance to its kth nearest neighbor (k-distance). Plot the k-distance for all data points in ascending order. Look for a knee or elbow point in the plot, which represents a significant increase in distance. This knee point can serve as an indication of an appropriate epsilon value.\n",
    "\n",
    "4. Reachability Plot: The reachability plot can assist in selecting the minimum points parameter value. Calculate the average reachability distance for each data point by considering its k nearest neighbors. Plot the reachability distance in descending order. Examine the plot and identify a threshold where the reachability distance increases sharply. This threshold can guide you in selecting a suitable value for the minimum points parameter.\n",
    "\n",
    "5. Evaluation Metrics: You can use evaluation metrics, such as the silhouette coefficient or the Davies-Bouldin index, to assess the quality of the clustering results for different parameter values. Iterate over a range of parameter values and calculate the evaluation metrics. Choose the parameter values that yield the best clustering performance according to the chosen evaluation metric.\n",
    "\n",
    "6. Grid Search: Perform a grid search over a predefined range of parameter values. Evaluate the clustering performance for each combination of parameter values using an appropriate evaluation metric. Select the parameter values that result in the highest clustering performance.\n",
    "\n",
    "It's important to note that determining the optimal parameter values in DBSCAN can be challenging, and the choice of parameter values depends on the specific dataset and problem at hand. It may require experimentation and iteration to find the most suitable parameter values that yield meaningful and reliable clustering results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q4. How does DBSCAN clustering handle outliers in a dataset?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering has a built-in mechanism to handle outliers in a dataset. The algorithm distinguishes outliers as data points that do not belong to any cluster. Here's how DBSCAN handles outliers:\n",
    "\n",
    "1. Core Points: In DBSCAN, a core point is defined as a data point that has a sufficient number of neighboring points within a specified distance called epsilon (ε). The minimum number of neighboring points required is determined by the minimum points parameter. Core points are considered as central points of dense regions in the dataset.\n",
    "\n",
    "2. Border Points: Border points are data points that are not core points themselves but are within the epsilon distance of at least one core point. These points are on the outskirts of a cluster and may have fewer neighboring points.\n",
    "\n",
    "3. Noise Points: Noise points, also known as outliers, are data points that are neither core points nor border points. These points do not have a sufficient number of neighboring points within the epsilon distance to be assigned to a cluster.\n",
    "\n",
    "During the clustering process, DBSCAN identifies and labels core points and border points as part of clusters based on their connectivity. Noise points are left unassigned, representing outliers.\n",
    "\n",
    "By considering the density of points, DBSCAN is able to separate outliers from the clustered data. Outliers are not forced into any cluster and are treated as distinct data points. This is particularly useful in applications where the detection and analysis of outliers are important, such as fraud detection or anomaly detection.\n",
    "\n",
    "It's important to note that the performance of DBSCAN in handling outliers depends on the choice of parameters, especially the epsilon (ε) and minimum points values. Setting appropriate values for these parameters is crucial to accurately identify outliers and capture the underlying cluster structures in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q5. How does DBSCAN clustering differ from k-means clustering?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering and k-means clustering are two distinct algorithms used for clustering data. Here are the key differences between DBSCAN and k-means clustering:\n",
    "\n",
    "1. Approach:\n",
    "   - DBSCAN: DBSCAN is a density-based clustering algorithm. It groups data points based on their density and connectivity. It identifies dense regions in the data and forms clusters by connecting points that are within a specified distance (epsilon) and have a minimum number of neighboring points.\n",
    "   - K-means: K-means is a centroid-based clustering algorithm. It aims to partition the data into a specified number of clusters, where each cluster is represented by the centroid (mean) of the data points belonging to that cluster. It iteratively assigns data points to the nearest centroid and updates the centroids until convergence.\n",
    "\n",
    "2. Cluster Shape and Size:\n",
    "   - DBSCAN: DBSCAN can identify clusters of arbitrary shapes and sizes. It can handle clusters that are non-linear, irregular, or have varying densities. It is effective in detecting clusters with complex structures.\n",
    "   - K-means: K-means assumes that clusters are convex and isotropic, meaning they have roughly equal sizes and densities. It may struggle to identify clusters of non-spherical or complex shapes.\n",
    "\n",
    "3. Number of Clusters:\n",
    "   - DBSCAN: DBSCAN does not require specifying the number of clusters in advance. It automatically determines the number of clusters based on the density and connectivity of the data. It can discover clusters of different sizes and adapt to the local density variations.\n",
    "   - K-means: K-means requires specifying the number of clusters before running the algorithm. The user must provide the desired number of clusters as a parameter. If the wrong number of clusters is chosen, it may lead to suboptimal results.\n",
    "\n",
    "4. Outlier Handling:\n",
    "   - DBSCAN: DBSCAN has a built-in mechanism to handle outliers or noise points. It can identify data points that do not belong to any cluster as outliers. It effectively separates outliers from the clustered data.\n",
    "   - K-means: K-means does not have an explicit mechanism to handle outliers. Outliers can significantly impact the centroid calculations and distort the cluster assignments.\n",
    "\n",
    "5. Parameter Sensitivity:\n",
    "   - DBSCAN: DBSCAN is sensitive to the choice of parameters, such as epsilon (ε) and minimum points. Selecting appropriate parameter values is crucial to obtain meaningful clusters.\n",
    "   - K-means: K-means is sensitive to the initial placement of centroids. Different initializations can lead to different results. It is common to run k-means multiple times with random initializations to mitigate this sensitivity.\n",
    "\n",
    "In summary, DBSCAN is a density-based algorithm that can handle clusters of arbitrary shapes, adapt to varying densities, automatically determine the number of clusters, and handle outliers effectively. On the other hand, k-means is a centroid-based algorithm that assumes convex and isotropic clusters, requires specifying the number of clusters, and can be sensitive to initializations. The choice between DBSCAN and k-means depends on the data characteristics, desired cluster shapes, presence of outliers, and the need for automatic cluster determination."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q6. Can DBSCAN clustering be applied to datasets with high dimensional feature spaces? If so, what are some potential challenges?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN clustering can be applied to datasets with high-dimensional feature spaces, but it can face some challenges. Here are a few considerations when applying DBSCAN to high-dimensional datasets:\n",
    "\n",
    "1. Curse of Dimensionality: High-dimensional spaces suffer from the \"curse of dimensionality.\" As the number of dimensions increases, the available data becomes sparse, and the distance between data points tends to become less meaningful. In high-dimensional spaces, the notion of density becomes less reliable, and the clustering performance of DBSCAN may degrade.\n",
    "\n",
    "2. Distance Metric Selection: The choice of distance metric becomes crucial in high-dimensional spaces. Euclidean distance, which is commonly used in DBSCAN, may not be effective in capturing the similarities between data points in high-dimensional spaces. Other distance metrics, such as cosine distance or Mahalanobis distance, may be more appropriate for specific data types or characteristics.\n",
    "\n",
    "3. Feature Selection or Dimensionality Reduction: In high-dimensional datasets, it can be beneficial to perform feature selection or dimensionality reduction techniques to reduce the number of dimensions. This helps in reducing the sparsity and noise in the data, improves the clustering performance, and mitigates the curse of dimensionality.\n",
    "\n",
    "4. Parameter Selection: The choice of parameters, specifically the epsilon (ε) and minimum points, becomes more challenging in high-dimensional spaces. Determining suitable parameter values is crucial to capture meaningful clusters while avoiding noise and obtaining a good balance between sparsity and density. Parameter selection techniques, such as the k-distance plot or evaluation metrics, can help in finding appropriate values.\n",
    "\n",
    "5. Visualization and Interpretation: Visualizing and interpreting high-dimensional data and the resulting clusters can be challenging. As the number of dimensions increases, it becomes difficult to visualize the data directly. Dimensionality reduction techniques, such as PCA or t-SNE, can be applied to project the data into lower-dimensional spaces for visualization and interpretation.\n",
    "\n",
    "6. Scalability: High-dimensional datasets can pose scalability challenges for DBSCAN. The computational complexity of DBSCAN increases with the number of data points and dimensions. Efficient implementation, optimization techniques, or using approximate versions of DBSCAN can help in improving the scalability for large high-dimensional datasets.\n",
    "\n",
    "In summary, DBSCAN can be applied to high-dimensional datasets, but it faces challenges related to the curse of dimensionality, distance metric selection, parameter selection, visualization, interpretation, and scalability. Preprocessing steps, careful parameter tuning, and appropriate data representations are essential to address these challenges and obtain meaningful clustering results in high-dimensional spaces."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q7. How does DBSCAN clustering handle clusters with varying densities?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering is well-suited for handling clusters with varying densities. Unlike other clustering algorithms that assume uniform densities, DBSCAN can adapt to local density variations within the data. Here's how DBSCAN handles clusters with varying densities:\n",
    "\n",
    "1. Core Points: In DBSCAN, a core point is defined as a data point that has a sufficient number of neighboring points within a specified distance called epsilon (ε). The minimum number of neighboring points required is determined by the minimum points parameter. Core points are considered as central points of dense regions in the dataset.\n",
    "\n",
    "2. Dense Regions: DBSCAN identifies dense regions by connecting core points and their directly reachable neighboring points. These dense regions can have varying sizes and densities. The algorithm does not assume a fixed density threshold for all clusters but adapts to the local density variations.\n",
    "\n",
    "3. Border Points: Border points are data points that are not core points themselves but are within the epsilon distance of at least one core point. These points are on the outskirts of a cluster and may have fewer neighboring points. Border points can help bridge regions of different densities and connect clusters with varying densities.\n",
    "\n",
    "4. Epsilon Parameter: The epsilon parameter plays a crucial role in capturing the density variations. By setting an appropriate epsilon value, DBSCAN can effectively differentiate between dense and sparse regions. A smaller epsilon value will result in smaller clusters and capture more fine-grained density variations, while a larger epsilon value will yield larger clusters and account for wider density variations.\n",
    "\n",
    "5. Handling Sparse Regions: DBSCAN can handle sparse regions in the data by considering them as noise or outliers. Data points that do not meet the density criteria to become core points or border points are classified as noise points. DBSCAN does not assign these points to any cluster, effectively handling sparse regions with low densities.\n",
    "\n",
    "By considering the density and connectivity of data points, DBSCAN can effectively identify clusters with varying densities. It is able to adapt to local density variations, capture dense regions of different sizes, and handle sparse regions by treating them as noise. This makes DBSCAN a powerful clustering algorithm for datasets where clusters exhibit different densities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q8. What are some common evaluation metrics used to assess the quality of DBSCAN clustering results?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several evaluation metrics can be used to assess the quality of DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering results. Here are some common evaluation metrics:\n",
    "\n",
    "1. Silhouette Coefficient: The silhouette coefficient measures the compactness and separation of clusters. It quantifies how well each data point fits into its assigned cluster compared to neighboring clusters. The silhouette coefficient ranges from -1 to 1, where a value closer to 1 indicates well-separated clusters, a value close to 0 suggests overlapping clusters, and a negative value indicates misclassified data points.\n",
    "\n",
    "2. Davies-Bouldin Index (DBI): The Davies-Bouldin index assesses the clustering quality by measuring the ratio of the average intra-cluster distance to the average inter-cluster distance. A lower DBI value indicates better-defined and well-separated clusters, with minimal overlap and maximum compactness.\n",
    "\n",
    "3. Calinski-Harabasz Index: The Calinski-Harabasz index evaluates the compactness and separation of clusters based on the ratio of between-cluster variance to within-cluster variance. A higher index value indicates better-defined clusters with high inter-cluster separation and low intra-cluster variance.\n",
    "\n",
    "4. Dunn Index: The Dunn index measures the compactness and separation of clusters by evaluating the ratio of the minimum inter-cluster distance to the maximum intra-cluster distance. A higher Dunn index value signifies better clustering results with well-separated clusters.\n",
    "\n",
    "5. Rand Index: The Rand index measures the similarity between the true class labels (if available) and the clustering results. It compares the pairs of data points and evaluates if they are assigned to the same cluster or not. A higher Rand index value indicates better agreement between the true labels and the clustering results.\n",
    "\n",
    "6. Jaccard Coefficient: The Jaccard coefficient is another metric that compares the similarity between the true class labels and the clustering results. It calculates the ratio of the number of data point pairs that are assigned to the same cluster in both the true and clustering results to the total number of data point pairs.\n",
    "\n",
    "These evaluation metrics can provide insights into the quality of the clustering results and help in comparing different parameter settings or algorithms. However, it's important to note that the choice of the most appropriate evaluation metric depends on the specific dataset, the availability of ground truth labels, and the goals of the clustering analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q9. Can DBSCAN clustering be used for semi-supervised learning tasks?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering is primarily an unsupervised learning algorithm, meaning it does not rely on labeled data for clustering. However, DBSCAN can be leveraged in certain scenarios to assist in semi-supervised learning tasks. Here's how DBSCAN can be used in a semi-supervised learning context:\n",
    "\n",
    "1. Pseudo-Labeling: After performing DBSCAN clustering on unlabeled data, you can assign cluster labels to the data points based on the clustering results. These cluster labels act as pseudo-labels, creating a partially labeled dataset. You can then train a supervised learning model using the partially labeled data, incorporating both the original labeled data and the pseudo-labeled data. This approach allows you to utilize the clustering structure to guide the semi-supervised learning process.\n",
    "\n",
    "2. Active Learning: DBSCAN can be used as an active learning strategy to select the most informative instances for labeling. By running DBSCAN on the unlabeled data, you can identify clusters with high density and select representative data points from these clusters for manual annotation. This way, DBSCAN helps in identifying the most uncertain or diverse instances for labeling, making the labeling process more efficient.\n",
    "\n",
    "3. Outlier Detection: DBSCAN's ability to identify outliers can be useful in semi-supervised learning. Outliers often represent anomalous or noisy data points. By removing these outliers from the unlabeled data or treating them separately, you can improve the quality of the semi-supervised learning process and reduce the influence of noisy instances.\n",
    "\n",
    "It's important to note that while DBSCAN can be applied in semi-supervised learning scenarios, it is not inherently designed for supervised learning tasks. The effectiveness of using DBSCAN in semi-supervised learning depends on the specific characteristics of the dataset and the underlying problem. In some cases, other semi-supervised learning techniques, such as co-training or self-training, may be more appropriate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q1O. How does DBSCAN clustering handle datasets with noise or missing values?`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering has some built-in mechanisms to handle datasets with noise or missing values. Here's how DBSCAN can handle these scenarios:\n",
    "\n",
    "1. Noise Handling: DBSCAN is designed to handle noise effectively. It identifies data points that do not belong to any cluster as noise points or outliers. These noise points are not assigned to any cluster and are considered separate from the clustered data. By treating noise as a distinct category, DBSCAN can accommodate datasets with noisy or irrelevant data points.\n",
    "\n",
    "2. Missing Values: DBSCAN can handle missing values by treating them as separate and distinct from the existing data points. When computing distances between data points, DBSCAN can consider missing values as a special value or use appropriate strategies such as pairwise deletion or imputation techniques to estimate missing values. It is important to preprocess the data and handle missing values appropriately before applying DBSCAN.\n",
    "\n",
    "3. Impact on Density: In DBSCAN, the density of a region is determined by the number of neighboring points within a specified distance (epsilon). If a data point has missing values or is treated as noise, it does not contribute to the density calculation for other points in the neighborhood. Therefore, the presence of missing values or noise can affect the density estimation and, consequently, the clustering results. Imputing missing values or handling noise properly is crucial to obtain meaningful clusters.\n",
    "\n",
    "4. Preprocessing: Before applying DBSCAN to a dataset with noise or missing values, it is advisable to preprocess the data. This may involve imputing missing values, removing or flagging noisy data points, or applying data cleaning techniques. Proper preprocessing helps in ensuring accurate density estimation and maintaining the integrity of the clustering process.\n",
    "\n",
    "It's important to note that the effectiveness of DBSCAN in handling noise or missing values depends on the specific characteristics of the dataset and the nature of the noise or missingness. Preprocessing steps, careful handling of missing values, and understanding the impact of noise on the clustering results are key considerations when working with datasets that have noise or missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Q11. Implement the DBSCAN algorithm using a python programming language, and apply it to a sample dataset. Discuss the clustering results and interpret the meaning of the obtained clusters.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels:\n",
      "[ 0  0  0  1  1  1  0  1  1  0  0  1  1  0  1  1  0  1  0  0  1  0  1  0\n",
      "  0  0  0  0  1  1  0  0  0  0  1  1  1  1  1  0  0  1  0  0  1  1  0  0\n",
      "  1  0  1  1  0  1  1  0  1  0  1  0  1  0  0  0  0  1  0  1  0  1  0  1\n",
      "  1  0  1  1  0  1 -1  1  0  0  0  1  1  0  1  0  1  0  0  1  1  0  0  1\n",
      "  1  0  0  1  0  0  1  1  1  1  0  0  1  1  1  0  1  1  0  1  0  0  1  0\n",
      "  0  1  1  1  0  0  1  1  0  0  1  0  0  1  0  1  0  1  0  1  1  1  1  0\n",
      "  0  0  0  1  0  0  1  1  0  0  0  0  1  1  0  0  1  1  1  1  0  0  1  0\n",
      "  1  0  0  0  0  0  1  1  1  0  1  1  0  0  1  1  1  1  0  1  0  0  1  1\n",
      "  0  1  0  1  0  0  1  1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "class DBSCAN:\n",
    "    def __init__(self, eps, min_pts):\n",
    "        self.eps = eps\n",
    "        self.min_pts = min_pts\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.labels_ = np.zeros(len(X), dtype=int)\n",
    "        self.visited = np.zeros(len(X), dtype=bool)\n",
    "        self.clusters = []\n",
    "        cluster_id = 0\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            if not self.visited[i]:\n",
    "                self.visited[i] = True\n",
    "                neighbors = self.get_neighbors(X, i)\n",
    "\n",
    "                if len(neighbors) < self.min_pts:\n",
    "                    self.labels_[i] = -1  # mark as noise\n",
    "                else:\n",
    "                    self.expand_cluster(X, i, neighbors, cluster_id)\n",
    "                    cluster_id += 1\n",
    "\n",
    "    def expand_cluster(self, X, i, neighbors, cluster_id):\n",
    "        self.labels_[i] = cluster_id\n",
    "        cluster = [i]\n",
    "\n",
    "        for j in neighbors:\n",
    "            if not self.visited[j]:\n",
    "                self.visited[j] = True\n",
    "                new_neighbors = self.get_neighbors(X, j)\n",
    "\n",
    "                if len(new_neighbors) >= self.min_pts:\n",
    "                    neighbors += new_neighbors\n",
    "\n",
    "            if self.labels_[j] == 0:\n",
    "                self.labels_[j] = cluster_id\n",
    "                cluster.append(j)\n",
    "\n",
    "        self.clusters.append(cluster)\n",
    "\n",
    "    def get_neighbors(self, X, i):\n",
    "        distances = pairwise_distances(X, [X[i]], metric='euclidean').flatten()\n",
    "        return np.where(distances <= self.eps)[0].tolist()\n",
    "\n",
    "# Sample dataset\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=42)\n",
    "\n",
    "# Data scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.3, min_pts=5)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Clustering results\n",
    "print(\"Cluster Labels:\")\n",
    "print(dbscan.labels_)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
